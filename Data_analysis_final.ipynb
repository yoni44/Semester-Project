{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9aba7ead-3c44-47de-a583-0f69b33f31f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA ANALYSIS & ML PREPARATION\n",
      "======================================================================\n",
      "Step 1: Loading Data\n",
      "\n",
      "âœ“ Loaded 8949 rows Ã— 41 columns\n",
      "âœ“ Metadata columns: 9\n",
      "âœ“ Numeric feature columns: 32\n",
      "\n",
      ">>> STEP 1 VERIFICATION:\n",
      "    âœ“ Data shape: (8949, 41)\n",
      "    âœ“ Substances: ['Acetone', 'Redidlo', 'Vinegar', 'Softasept', 'Savo']\n",
      "    âœ“ Measurement types: ['acetone', 'air', 'redidlo', 'vinegar', 'softasept', 'savo']\n",
      "Step 2: Detailed Statistical Analysis\n",
      "\n",
      " 2.1 Basic Statistics \n",
      "  Total Samples: 8949\n",
      "  Total Features: 32\n",
      "  Substances: 5\n",
      "  Missing Values (%): 10.89\n",
      "\n",
      " 2.2 Per-Substance Statistics \n",
      "Substance  Total_Samples  Air_Samples  Substance_Samples  Air_Ratio  Mean_Gas_Res  Std_Gas_Res  CV_Gas_Res\n",
      "  Acetone           2203         1047               1156  47.526101  39968.597133 42840.349333  107.185021\n",
      "  Redidlo           2071         1070               1001  51.665862  44096.985010 44936.049527  101.902771\n",
      "  Vinegar           1679          860                819  51.220965  47950.617452 50363.474700  105.031963\n",
      "Softasept           1464          727                737  49.658470  21299.512450 28709.609460  134.789984\n",
      "     Savo           1532          814                718  53.133159  50920.935613 54221.782259  106.482298\n",
      "\n",
      " 2.3 Feature Statistics \n",
      "âœ“ Saved detailed feature statistics (32 features)\n",
      "\n",
      " 2.4 ANOVA Tests for Feature Significance \n",
      "\n",
      "Top 10 Most Discriminative Features (ANOVA):\n",
      "      Feature   F_Statistic  P_Value  Significant\n",
      "bme688_4_pres 203304.334891      0.0         True\n",
      "bme688_6_pres 199101.053950      0.0         True\n",
      "bme688_2_pres 197268.767930      0.0         True\n",
      "bme688_3_pres 196212.184614      0.0         True\n",
      "bme688_1_pres 196120.749310      0.0         True\n",
      "bme688_7_pres 140063.948020      0.0         True\n",
      "bme688_8_pres  61922.672993      0.0         True\n",
      "bme688_5_pres  47473.606813      0.0         True\n",
      "bme688_4_temp   4873.254396      0.0         True\n",
      "bme688_1_temp   4670.599047      0.0         True\n",
      "\n",
      ">>>PRINT VERIFICATION:\n",
      "    âœ“ Substance statistics saved: detailed_substance_statistics.csv\n",
      "    âœ“ Feature statistics saved: detailed_feature_statistics.csv\n",
      "    âœ“ ANOVA results saved: anova_results.csv\n",
      "    âœ“ Significant features (p<0.05): 32/32\n",
      "STEP 3: Feature Importance & Analysis\n",
      "\n",
      " 3.1 Mutual Information Scores \n",
      "Top 10 Features by Mutual Information:\n",
      "      Feature  MI_Score\n",
      "bme688_4_pres  1.420061\n",
      "bme688_3_pres  1.411966\n",
      "bme688_7_pres  1.410155\n",
      "bme688_8_pres  1.408069\n",
      "bme688_5_pres  1.405517\n",
      "bme688_1_pres  1.393870\n",
      "bme688_2_pres  1.392705\n",
      "bme688_6_pres  1.388494\n",
      " bme688_8_hum  1.204746\n",
      " bme688_7_hum  1.178722\n",
      "\n",
      " 3.2 F-Scores \n",
      "Top 10 Features by F-Score:\n",
      "      Feature     F_Score  P_Value\n",
      "bme688_3_pres 8971.858772      0.0\n",
      "bme688_2_pres 8965.610838      0.0\n",
      "bme688_6_pres 8953.568841      0.0\n",
      "bme688_1_pres 8930.196318      0.0\n",
      "bme688_7_pres 8799.002644      0.0\n",
      "bme688_5_pres 7898.124801      0.0\n",
      "bme688_4_pres 7808.584556      0.0\n",
      "bme688_8_pres 7211.819295      0.0\n",
      "bme688_4_temp 3356.704006      0.0\n",
      "bme688_1_temp 3187.127809      0.0\n",
      "\n",
      " 3.3 Combined Feature Ranking \n",
      "Top 15 Features (Combined Ranking):\n",
      "      Feature  MI_Score     F_Score  Avg_Rank\n",
      "bme688_3_pres  1.411966 8971.858772       1.5\n",
      "bme688_4_pres  1.420061 7808.584556       4.0\n",
      "bme688_7_pres  1.410155 8799.002644       4.0\n",
      "bme688_2_pres  1.392705 8965.610838       4.5\n",
      "bme688_1_pres  1.393870 8930.196318       5.0\n",
      "bme688_5_pres  1.405517 7898.124801       5.5\n",
      "bme688_6_pres  1.388494 8953.568841       5.5\n",
      "bme688_8_pres  1.408069 7211.819295       6.0\n",
      "bme688_1_temp  1.168888 3187.127809      11.5\n",
      "bme688_4_temp  1.161996 3356.704006      12.0\n",
      "bme688_2_temp  1.173132 3107.849856      14.0\n",
      "bme688_3_temp  1.153603 3180.036564      14.5\n",
      " bme688_8_hum  1.204746 2794.844758      15.0\n",
      " bme688_6_hum  1.173797 2825.222054      15.5\n",
      "bme688_6_temp  1.145545 3163.898922      15.5\n",
      "\n",
      ">>> PRINT VERIFICATION:\n",
      "    âœ“ Mutual Information scores saved\n",
      "    âœ“ F-Scores saved\n",
      "    âœ“ Combined ranking saved\n",
      "    âœ“ Top feature: bme688_3_pres\n",
      "STEP 4: Dimensionality Reduction & Class Separability\n",
      "\n",
      " 4.1 PCA Analysis \n",
      "Explained Variance by Component:\n",
      "  PC1: 40.92% (Cumulative: 40.92%)\n",
      "  PC2: 20.53% (Cumulative: 61.46%)\n",
      "  PC3: 18.17% (Cumulative: 79.62%)\n",
      "  PC4: 13.82% (Cumulative: 93.45%)\n",
      "  PC5: 1.58% (Cumulative: 95.03%)\n",
      "âœ“ Saved: 09_pca_analysis.png\n",
      "\n",
      " 4.2 t-SNE Analysis \n",
      "  Running t-SNE with perplexity=5...\n",
      "  Running t-SNE with perplexity=30...\n",
      "  Running t-SNE with perplexity=50...\n",
      "âœ“ Saved: 10_tsne_analysis.png\n",
      "\n",
      "  4.3 UMAP Analysis  \n",
      "  Running UMAP with n_neighbors=5...\n",
      "  Running UMAP with n_neighbors=15...\n",
      "  Running UMAP with n_neighbors=50...\n",
      "âœ“ Saved: 11_umap_analysis.png\n",
      "\n",
      " 4.4 Class Separability Metrics \n",
      "Class Separability Metrics:\n",
      "  Silhouette_Score: 0.3072\n",
      "  Calinski_Harabasz_Index: 1618.0656\n",
      "  Davies_Bouldin_Index: 1.2998\n",
      "  Fisher_Criterion: 19.8504\n",
      "\n",
      " 4.5 Combined Visualization \n",
      "âœ“ Saved: 12_combined_separability.png\n",
      "\n",
      ">>> PRINT VERIFICATION:\n",
      "    âœ“ PCA analysis saved: 09_pca_analysis.png\n",
      "    âœ“ t-SNE analysis saved: 10_tsne_analysis.png\n",
      "    âœ“ UMAP analysis saved: 11_umap_analysis.png\n",
      "    âœ“ Combined visualization saved: 12_combined_separability.png\n",
      "    âœ“ Silhouette Score: 0.3072\n",
      "STEP 5: Feature Engineering\n",
      "\n",
      " 5.1 Gas Resistance Features \n",
      "  Created 8 gas resistance aggregate features\n",
      "\n",
      " 5.2 Temperature & Humidity Features \n",
      "  Created temp_mean feature\n",
      "  Created hum_mean feature\n",
      "\n",
      " 5.3 Sensor Profile Features \n",
      "  Created gas_res_slope feature\n",
      "\n",
      "  Total engineered features: 11\n",
      "\n",
      ">>> PRINT VERIFICATION:\n",
      "    âœ“ Engineered features: ['gas_res_mean', 'gas_res_std', 'gas_res_min', 'gas_res_max', 'gas_res_range', 'gas_res_cv', 'gas_res_ratio_1_8', 'gas_res_ratio_max_min', 'temp_mean', 'hum_mean', 'gas_res_slope']\n",
      "STEP 6: Prepare ML-Ready Data Structures\n",
      "\n",
      "  Total features: 43\n",
      "  Valid features (< 30% missing): 43\n",
      "\n",
      " 6.1 Train/Validation/Test Split \n",
      "  Training set: 2658 samples (60.0%)\n",
      "  Validation set: 886 samples (20.0%)\n",
      "  Test set: 887 samples (20.0%)\n",
      "\n",
      " 6.2 Saving ML-Ready Datasets \n",
      "  âœ“ Saved: ml_train_final.csv\n",
      "  âœ“ Saved: ml_val_final.csv\n",
      "  âœ“ Saved: ml_test_final.csv\n",
      "  âœ“ Saved: ml_full_scaled.csv\n",
      "  âœ“ Saved: ml_full_unscaled.csv\n",
      "\n",
      " 6.3 Saving Metadata \n",
      "  âœ“ Saved: ml_metadata.json\n",
      "  âœ“ Saved: scaler.pkl, label_encoder.pkl, imputer.pkl\n",
      "\n",
      " 6.4 Creating K-Fold Cross-Validation Indices \n",
      "  âœ“ Saved: kfold_indices.json (5-fold CV)\n",
      "\n",
      ">>> SECTION 6 VERIFICATION:\n",
      "    âœ“ Training samples: 2658\n",
      "    âœ“ Validation samples: 886\n",
      "    âœ“ Test samples: 887\n",
      "    âœ“ Features: 43\n",
      "    âœ“ Classes: ['Acetone', 'Redidlo', 'Savo', 'Softasept', 'Vinegar']\n",
      "SECTION 7: Data Quality Report\n",
      "\n",
      "Data Quality Summary:\n",
      "  Total Samples: 8949\n",
      "  Final Features: 43\n",
      "  Missing Data: 10.89%\n",
      "  Class Imbalance Ratio: 1.61\n",
      "  Silhouette Score: 0.3072\n",
      "\n",
      "Recommendations:\n",
      "\n",
      ">>> SECTION 7 VERIFICATION:\n",
      "    âœ“ Data quality report saved: data_quality_report.json\n",
      "\n",
      "======================================================================\n",
      "STEP-3 FINAL SUMMARY\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ All outputs saved to: C:\\Users\\yonat\\OneDrive\\Desktop\\Semester Project\\Measurements\\output\n",
      "\n",
      "ğŸ“Š STATISTICAL ANALYSIS:\n",
      "   â€¢ detailed_substance_statistics.csv\n",
      "   â€¢ detailed_feature_statistics.csv\n",
      "   â€¢ anova_results.csv\n",
      "\n",
      "ğŸ“ˆ FEATURE IMPORTANCE:\n",
      "   â€¢ mutual_information_scores.csv\n",
      "   â€¢ f_scores.csv\n",
      "   â€¢ combined_feature_ranking.csv\n",
      "\n",
      "ğŸ–¼ï¸  VISUALIZATIONS:\n",
      "   â€¢ 09_pca_analysis.png\n",
      "   â€¢ 10_tsne_analysis.png\n",
      "   â€¢ 11_umap_analysis.png\n",
      "   â€¢ 12_combined_separability.png\n",
      "\n",
      "ğŸ¤– ML-READY DATA:\n",
      "   â€¢ ml_train_final.csv (60% of data)\n",
      "   â€¢ ml_val_final.csv (20% of data)\n",
      "   â€¢ ml_test_final.csv (20% of data)\n",
      "   â€¢ ml_full_scaled.csv (standardized)\n",
      "   â€¢ ml_full_unscaled.csv (for tree models)\n",
      "   â€¢ ml_metadata.json\n",
      "   â€¢ kfold_indices.json\n",
      "   â€¢ scaler.pkl, label_encoder.pkl, imputer.pkl\n",
      "\n",
      "ğŸ“‹ QUALITY REPORT:\n",
      "   â€¢ data_quality_report.json\n",
      "\n",
      "======================================================================\n",
      ">>> FINAL VERIFICATION: ALL SECTIONS PASSED âœ“\n",
      "======================================================================\n",
      "STEP-3 COMPLETE - Data is ready for ML model training! ğŸš€\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data Analysis & ML Preparation\n",
    "================================================\n",
    "This Code performs:\n",
    "1. Detailed Statistical Analysis\n",
    "2. Feature and class separability (PCA, t-SNE, UMAP)\n",
    "3. Proper ML Data Structure Preparation\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif, SelectKBest\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import UMAP\n",
    "try:\n",
    "    import umap\n",
    "    UMAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    UMAP_AVAILABLE = False\n",
    "    print(\"Warning: UMAP not installed. Run: pip install umap-learn\")\n",
    "\n",
    "\n",
    "# The input directory and final data directory or output directory\n",
    "DATA_DIR = \"./output\"\n",
    "INPUT_FILE = \"cleaned_measurements.csv\"\n",
    "SUBSTANCES = ['Acetone', 'Redidlo', 'Vinegar', 'Softasept', 'Savo']\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "#Let us use Helper functions to Convert nested JSON sensor data into a flat dictionary for tabular analysis\n",
    "def calculate_class_separability(X, y):\n",
    "    \"\"\"Calculate Fisher's Linear Discriminant ratio for class separability\"\"\" #to identify how well the class separtaed\n",
    "    classes = np.unique(y)\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    # Overall mean\n",
    "    overall_mean = np.mean(X, axis=0)\n",
    "    \n",
    "    # Between-class scatter\n",
    "    Sb = np.zeros((n_features, n_features))\n",
    "    # Within-class scatter\n",
    "    Sw = np.zeros((n_features, n_features))\n",
    "    \n",
    "    for c in classes:\n",
    "        X_c = X[y == c]\n",
    "        mean_c = np.mean(X_c, axis=0)\n",
    "        n_c = X_c.shape[0]\n",
    "        \n",
    "        # Between-class\n",
    "        mean_diff = (mean_c - overall_mean).reshape(-1, 1)\n",
    "        Sb += n_c * np.dot(mean_diff, mean_diff.T)\n",
    "        \n",
    "        # Within-class\n",
    "        for x in X_c:\n",
    "            diff = (x - mean_c).reshape(-1, 1)\n",
    "            Sw += np.dot(diff, diff.T)\n",
    "    \n",
    "    # Fisher criterion (trace ratio)\n",
    "    try:\n",
    "        Sw_inv = np.linalg.pinv(Sw)\n",
    "        fisher_ratio = np.trace(np.dot(Sw_inv, Sb))\n",
    "    except:\n",
    "        fisher_ratio = np.nan\n",
    "    \n",
    "    return fisher_ratio\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"DATA ANALYSIS & ML PREPARATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Setup paths\n",
    "    data_path = Path(DATA_DIR)\n",
    "    input_file = data_path / INPUT_FILE\n",
    "    \n",
    "    if not input_file.exists():\n",
    "        print(f\"\\nâŒ ERROR: Input file not found: {input_file}\")\n",
    "        return None\n",
    "\n",
    "    print(\"Step 1: Loading Data\")\n",
    "\n",
    "    \n",
    "    df = pd.read_csv(input_file)\n",
    "    print(f\"\\nâœ“ Loaded {len(df)} rows Ã— {len(df.columns)} columns\")\n",
    "    \n",
    "    # Identify columns\n",
    "    meta_cols = ['id', 'box_id', 'ts', 'measurement_id', 'substance', \n",
    "                 'measurement_type', 'measurement_num', 'source_file', 'created_at']\n",
    "    available_meta = [c for c in meta_cols if c in df.columns]\n",
    "    numeric_cols = [c for c in df.columns if c not in available_meta]\n",
    "    \n",
    "    # Convert to numeric\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    print(f\"âœ“ Metadata columns: {len(available_meta)}\")\n",
    "    print(f\"âœ“ Numeric feature columns: {len(numeric_cols)}\")\n",
    "    \n",
    "    # --- VERIFICATION ---\n",
    "    print(\"\\n>>> STEP 1 VERIFICATION:\")\n",
    "    print(f\"    âœ“ Data shape: {df.shape}\")\n",
    "    print(f\"    âœ“ Substances: {df['substance'].unique().tolist()}\")\n",
    "    print(f\"    âœ“ Measurement types: {df['measurement_type'].unique().tolist()}\")\n",
    "    \n",
    "    print(\"Step 2: Detailed Statistical Analysis\")\n",
    "    # 2.1 Basic Statistics\n",
    "    print(\"\\n 2.1 Basic Statistics \")\n",
    "    \n",
    "    basic_stats = {\n",
    "        'Total Samples': len(df),\n",
    "        'Total Features': len(numeric_cols),\n",
    "        'Substances': len(df['substance'].unique()),\n",
    "        'Missing Values (%)': (df[numeric_cols].isnull().sum().sum() / (len(df) * len(numeric_cols)) * 100),\n",
    "    }\n",
    "    \n",
    "    for key, value in basic_stats.items():\n",
    "        print(f\"  {key}: {value:.2f}\" if isinstance(value, float) else f\"  {key}: {value}\")\n",
    "    \n",
    "\n",
    "    print(\"\\n 2.2 Per-Substance Statistics \")\n",
    "    substance_stats = []\n",
    "    for substance in df['substance'].unique():\n",
    "        subset = df[df['substance'] == substance]\n",
    "        air_subset = subset[subset['measurement_type'] == 'air']\n",
    "        subst_subset = subset[subset['measurement_type'] != 'air']\n",
    "        \n",
    "        stats_dict = {\n",
    "            'Substance': substance,\n",
    "            'Total_Samples': len(subset),\n",
    "            'Air_Samples': len(air_subset),\n",
    "            'Substance_Samples': len(subst_subset),\n",
    "            'Air_Ratio': len(air_subset) / len(subset) * 100 if len(subset) > 0 else 0,\n",
    "        }\n",
    "        \n",
    "        # Key feature statistics (for gas resistance)\n",
    "        gas_res_cols = [c for c in numeric_cols if 'gas_res' in c]\n",
    "        if gas_res_cols:\n",
    "            gas_res_mean = subst_subset[gas_res_cols].mean().mean()\n",
    "            gas_res_std = subst_subset[gas_res_cols].std().mean()\n",
    "            stats_dict['Mean_Gas_Res'] = gas_res_mean\n",
    "            stats_dict['Std_Gas_Res'] = gas_res_std\n",
    "            stats_dict['CV_Gas_Res'] = (gas_res_std / gas_res_mean * 100) if gas_res_mean > 0 else 0\n",
    "        \n",
    "        substance_stats.append(stats_dict)\n",
    "    \n",
    "    substance_stats_df = pd.DataFrame(substance_stats)\n",
    "    print(substance_stats_df.to_string(index=False))\n",
    "    substance_stats_df.to_csv(data_path / 'detailed_substance_statistics.csv', index=False)\n",
    "    \n",
    "    # 2.3 Feature Statistics\n",
    "    print(\"\\n 2.3 Feature Statistics \")\n",
    "    \n",
    "    feature_stats = []\n",
    "    for col in numeric_cols:\n",
    "        col_data = df[col].dropna()\n",
    "        if len(col_data) > 0:\n",
    "            stats_dict = {\n",
    "                'Feature': col,\n",
    "                'Count': len(col_data),\n",
    "                'Missing_Pct': (df[col].isnull().sum() / len(df) * 100),\n",
    "                'Mean': col_data.mean(),\n",
    "                'Std': col_data.std(),\n",
    "                'Min': col_data.min(),\n",
    "                'Q25': col_data.quantile(0.25),\n",
    "                'Median': col_data.median(),\n",
    "                'Q75': col_data.quantile(0.75),\n",
    "                'Max': col_data.max(),\n",
    "                'Skewness': stats.skew(col_data),\n",
    "                'Kurtosis': stats.kurtosis(col_data),\n",
    "            }\n",
    "            feature_stats.append(stats_dict)\n",
    "    \n",
    "    feature_stats_df = pd.DataFrame(feature_stats)\n",
    "    feature_stats_df.to_csv(data_path / 'detailed_feature_statistics.csv', index=False)\n",
    "    print(f\"âœ“ Saved detailed feature statistics ({len(feature_stats)} features)\")\n",
    "    \n",
    "    print(\"\\n 2.4 ANOVA Tests for Feature Significance \")\n",
    "    # Filter to substance-only measurements\n",
    "    df_subst = df[df['measurement_type'] != 'air'].copy()\n",
    "    \n",
    "    anova_results = []\n",
    "    for col in numeric_cols:\n",
    "        col_data = df_subst[col].dropna()\n",
    "        if len(col_data) > 100:  # Minimum samples\n",
    "            groups = [df_subst[df_subst['substance'] == s][col].dropna() for s in df_subst['substance'].unique()]\n",
    "            groups = [g for g in groups if len(g) > 10]\n",
    "            \n",
    "            if len(groups) >= 2:\n",
    "                try:\n",
    "                    f_stat, p_value = stats.f_oneway(*groups)\n",
    "                    anova_results.append({\n",
    "                        'Feature': col,\n",
    "                        'F_Statistic': f_stat,\n",
    "                        'P_Value': p_value,\n",
    "                        'Significant': p_value < 0.05\n",
    "                    })\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    anova_df = pd.DataFrame(anova_results).sort_values('F_Statistic', ascending=False)\n",
    "    anova_df.to_csv(data_path / 'anova_results.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 Most Discriminative Features (ANOVA):\")\n",
    "    print(anova_df.head(10).to_string(index=False))\n",
    "    \n",
    "    print(\"\\n>>>PRINT VERIFICATION:\")\n",
    "    print(f\"    âœ“ Substance statistics saved: detailed_substance_statistics.csv\")\n",
    "    print(f\"    âœ“ Feature statistics saved: detailed_feature_statistics.csv\")\n",
    "    print(f\"    âœ“ ANOVA results saved: anova_results.csv\")\n",
    "    print(f\"    âœ“ Significant features (p<0.05): {anova_df['Significant'].sum()}/{len(anova_df)}\")\n",
    "\n",
    "\n",
    "    print(\"STEP 3: Feature Importance & Analysis\") \n",
    "    # Prepare data for feature importance\n",
    "    df_analysis = df_subst.copy()\n",
    "    X_raw = df_analysis[numeric_cols].copy()\n",
    "    \n",
    "    # Impute missing values\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_imputed = pd.DataFrame(imputer.fit_transform(X_raw), columns=numeric_cols, index=X_raw.index)\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df_analysis['substance'])\n",
    "    \n",
    "    # 3.1 Mutual Information\n",
    "    print(\"\\n 3.1 Mutual Information Scores \")\n",
    "    \n",
    "    mi_scores = mutual_info_classif(X_imputed, y, random_state=42)\n",
    "    mi_df = pd.DataFrame({\n",
    "        'Feature': numeric_cols,\n",
    "        'MI_Score': mi_scores\n",
    "    }).sort_values('MI_Score', ascending=False)\n",
    "    mi_df.to_csv(data_path / 'mutual_information_scores.csv', index=False)\n",
    "    \n",
    "    print(\"Top 10 Features by Mutual Information:\")\n",
    "    print(mi_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # 3.2 F-Score (ANOVA F-value)\n",
    "    print(\"\\n 3.2 F-Scores \")\n",
    "    \n",
    "    f_scores, f_pvalues = f_classif(X_imputed, y)\n",
    "    f_df = pd.DataFrame({\n",
    "        'Feature': numeric_cols,\n",
    "        'F_Score': f_scores,\n",
    "        'P_Value': f_pvalues\n",
    "    }).sort_values('F_Score', ascending=False)\n",
    "    f_df.to_csv(data_path / 'f_scores.csv', index=False)\n",
    "    \n",
    "    print(\"Top 10 Features by F-Score:\")\n",
    "    print(f_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # 3.3 Combined Feature Ranking\n",
    "    print(\"\\n 3.3 Combined Feature Ranking \")\n",
    "    \n",
    "    # Normalize scores\n",
    "    mi_df['MI_Rank'] = mi_df['MI_Score'].rank(ascending=False)\n",
    "    f_df['F_Rank'] = f_df['F_Score'].rank(ascending=False)\n",
    "    \n",
    "    combined_df = mi_df.merge(f_df[['Feature', 'F_Score', 'F_Rank']], on='Feature')\n",
    "    combined_df['Avg_Rank'] = (combined_df['MI_Rank'] + combined_df['F_Rank']) / 2\n",
    "    combined_df = combined_df.sort_values('Avg_Rank')\n",
    "    combined_df.to_csv(data_path / 'combined_feature_ranking.csv', index=False)\n",
    "    \n",
    "    print(\"Top 15 Features (Combined Ranking):\")\n",
    "    print(combined_df.head(15)[['Feature', 'MI_Score', 'F_Score', 'Avg_Rank']].to_string(index=False))\n",
    "    \n",
    "    # VERIFICATION \n",
    "    print(\"\\n>>> PRINT VERIFICATION:\")\n",
    "    print(f\"    âœ“ Mutual Information scores saved\")\n",
    "    print(f\"    âœ“ F-Scores saved\")\n",
    "    print(f\"    âœ“ Combined ranking saved\")\n",
    "    print(f\"    âœ“ Top feature: {combined_df.iloc[0]['Feature']}\")\n",
    "    \n",
    "    print(\"STEP 4: Dimensionality Reduction & Class Separability\")\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_imputed)\n",
    "    \n",
    "    # Get substance labels for coloring\n",
    "    substances = df_analysis['substance'].values\n",
    "    \n",
    "    # Color palette\n",
    "    colors = {'Acetone': '#E91E63', 'Redidlo': '#FFC107', 'Vinegar': '#4CAF50', \n",
    "              'Softasept': '#00BCD4', 'Savo': '#3F51B5'}\n",
    "    color_list = [colors.get(s, '#000000') for s in substances]\n",
    "    \n",
    "    # 4.1 PCA Analysis\n",
    "    print(\"\\n 4.1 PCA Analysis \")\n",
    "    \n",
    "    pca = PCA(n_components=min(10, X_scaled.shape[1]))\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Explained variance\n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "    cumulative_var = np.cumsum(explained_var)\n",
    "    \n",
    "    print(f\"Explained Variance by Component:\")\n",
    "    for i in range(min(5, len(explained_var))):\n",
    "        print(f\"  PC{i+1}: {explained_var[i]*100:.2f}% (Cumulative: {cumulative_var[i]*100:.2f}%)\")\n",
    "    \n",
    "    # PCA Plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Scree plot\n",
    "    axes[0].bar(range(1, len(explained_var)+1), explained_var * 100, alpha=0.7, label='Individual')\n",
    "    axes[0].plot(range(1, len(explained_var)+1), cumulative_var * 100, 'ro-', label='Cumulative')\n",
    "    axes[0].axhline(y=95, color='g', linestyle='--', label='95% threshold')\n",
    "    axes[0].set_xlabel('Principal Component')\n",
    "    axes[0].set_ylabel('Explained Variance (%)')\n",
    "    axes[0].set_title('PCA Scree Plot', fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # PC1 vs PC2\n",
    "    for substance in SUBSTANCES:\n",
    "        mask = substances == substance\n",
    "        axes[1].scatter(X_pca[mask, 0], X_pca[mask, 1], c=colors[substance], \n",
    "                       label=substance, alpha=0.6, s=30)\n",
    "    axes[1].set_xlabel(f'PC1 ({explained_var[0]*100:.1f}%)')\n",
    "    axes[1].set_ylabel(f'PC2 ({explained_var[1]*100:.1f}%)')\n",
    "    axes[1].set_title('PCA: PC1 vs PC2', fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # PC1 vs PC3\n",
    "    for substance in SUBSTANCES:\n",
    "        mask = substances == substance\n",
    "        axes[2].scatter(X_pca[mask, 0], X_pca[mask, 2], c=colors[substance], \n",
    "                       label=substance, alpha=0.6, s=30)\n",
    "    axes[2].set_xlabel(f'PC1 ({explained_var[0]*100:.1f}%)')\n",
    "    axes[2].set_ylabel(f'PC3 ({explained_var[2]*100:.1f}%)')\n",
    "    axes[2].set_title('PCA: PC1 vs PC3', fontweight='bold')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(data_path / '09_pca_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"âœ“ Saved: 09_pca_analysis.png\")\n",
    "    \n",
    "    # PCA loadings\n",
    "    loadings = pd.DataFrame(pca.components_.T, columns=[f'PC{i+1}' for i in range(pca.n_components_)],\n",
    "                           index=numeric_cols)\n",
    "    loadings.to_csv(data_path / 'pca_loadings.csv')\n",
    "    \n",
    "    # 4.2 t-SNE Analysis\n",
    "    print(\"\\n 4.2 t-SNE Analysis \")\n",
    "    \n",
    "    # Use PCA-reduced data for t-SNE (faster and often better)\n",
    "    n_components_tsne = min(50, X_pca.shape[1])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    perplexities = [5, 30, 50]\n",
    "    for idx, perp in enumerate(perplexities):\n",
    "        print(f\"  Running t-SNE with perplexity={perp}...\")\n",
    "        tsne = TSNE(n_components=2, perplexity=perp, random_state=42, max_iter=1000)\n",
    "        X_tsne = tsne.fit_transform(X_pca[:, :n_components_tsne])\n",
    "        \n",
    "        for substance in SUBSTANCES:\n",
    "            mask = substances == substance\n",
    "            axes[idx].scatter(X_tsne[mask, 0], X_tsne[mask, 1], c=colors[substance], \n",
    "                            label=substance, alpha=0.6, s=30)\n",
    "        axes[idx].set_xlabel('t-SNE 1')\n",
    "        axes[idx].set_ylabel('t-SNE 2')\n",
    "        axes[idx].set_title(f't-SNE (perplexity={perp})', fontweight='bold')\n",
    "        axes[idx].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(data_path / '10_tsne_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"âœ“ Saved: 10_tsne_analysis.png\")\n",
    "    \n",
    "    # 4.3 UMAP Analysis\n",
    "    if UMAP_AVAILABLE:\n",
    "        print(\"\\n  4.3 UMAP Analysis  \")\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        n_neighbors_list = [5, 15, 50]\n",
    "        for idx, n_neighbors in enumerate(n_neighbors_list):\n",
    "            print(f\"  Running UMAP with n_neighbors={n_neighbors}...\")\n",
    "            reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=0.1, random_state=42)\n",
    "            X_umap = reducer.fit_transform(X_scaled)\n",
    "            \n",
    "            for substance in SUBSTANCES:\n",
    "                mask = substances == substance\n",
    "                axes[idx].scatter(X_umap[mask, 0], X_umap[mask, 1], c=colors[substance], \n",
    "                                label=substance, alpha=0.6, s=30)\n",
    "            axes[idx].set_xlabel('UMAP 1')\n",
    "            axes[idx].set_ylabel('UMAP 2')\n",
    "            axes[idx].set_title(f'UMAP (n_neighbors={n_neighbors})', fontweight='bold')\n",
    "            axes[idx].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(data_path / '11_umap_analysis.png', dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"âœ“ Saved: 11_umap_analysis.png\")\n",
    "    else:\n",
    "        print(\"\\n 4.3 UMAP Analysis (SKIPPED - not available) \")\n",
    "    \n",
    "    # 4.4 Class Separability Metrics\n",
    "    print(\"\\n 4.4 Class Separability Metrics \")\n",
    "    \n",
    "    # Silhouette-like metric per class\n",
    "    from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "    \n",
    "    separability_metrics = {\n",
    "        'Silhouette_Score': silhouette_score(X_scaled, y),\n",
    "        'Calinski_Harabasz_Index': calinski_harabasz_score(X_scaled, y),\n",
    "        'Davies_Bouldin_Index': davies_bouldin_score(X_scaled, y),\n",
    "        'Fisher_Criterion': calculate_class_separability(X_scaled, y)\n",
    "    }\n",
    "    \n",
    "    print(\"Class Separability Metrics:\")\n",
    "    for metric, value in separability_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    with open(data_path / 'class_separability_metrics.json', 'w') as f:\n",
    "        json.dump({k: float(v) for k, v in separability_metrics.items()}, f, indent=2)\n",
    "    \n",
    "    # 4.5 Combined Visualization\n",
    "    print(\"\\n 4.5 Combined Visualization \")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 14))\n",
    "    \n",
    "    # PCA\n",
    "    for substance in SUBSTANCES:\n",
    "        mask = substances == substance\n",
    "        axes[0, 0].scatter(X_pca[mask, 0], X_pca[mask, 1], c=colors[substance], \n",
    "                          label=substance, alpha=0.6, s=30)\n",
    "    axes[0, 0].set_xlabel(f'PC1 ({explained_var[0]*100:.1f}%)')\n",
    "    axes[0, 0].set_ylabel(f'PC2 ({explained_var[1]*100:.1f}%)')\n",
    "    axes[0, 0].set_title('PCA', fontweight='bold', fontsize=14)\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # t-SNE (perplexity=30)\n",
    "    tsne = TSNE(n_components=2, perplexity=30, random_state=42, max_iter=1000)\n",
    "    X_tsne = tsne.fit_transform(X_pca[:, :n_components_tsne])\n",
    "    for substance in SUBSTANCES:\n",
    "        mask = substances == substance\n",
    "        axes[0, 1].scatter(X_tsne[mask, 0], X_tsne[mask, 1], c=colors[substance], \n",
    "                          label=substance, alpha=0.6, s=30)\n",
    "    axes[0, 1].set_xlabel('t-SNE 1')\n",
    "    axes[0, 1].set_ylabel('t-SNE 2')\n",
    "    axes[0, 1].set_title('t-SNE (perplexity=30)', fontweight='bold', fontsize=14)\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # UMAP\n",
    "    if UMAP_AVAILABLE:\n",
    "        reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "        X_umap = reducer.fit_transform(X_scaled)\n",
    "        for substance in SUBSTANCES:\n",
    "            mask = substances == substance\n",
    "            axes[1, 0].scatter(X_umap[mask, 0], X_umap[mask, 1], c=colors[substance], \n",
    "                              label=substance, alpha=0.6, s=30)\n",
    "        axes[1, 0].set_xlabel('UMAP 1')\n",
    "        axes[1, 0].set_ylabel('UMAP 2')\n",
    "        axes[1, 0].set_title('UMAP (n_neighbors=15)', fontweight='bold', fontsize=14)\n",
    "        axes[1, 0].legend()\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'UMAP not available', ha='center', va='center', fontsize=14)\n",
    "        axes[1, 0].set_title('UMAP', fontweight='bold', fontsize=14)\n",
    "    \n",
    "    # Class distribution\n",
    "    class_counts = df_analysis['substance'].value_counts()\n",
    "    axes[1, 1].bar(class_counts.index, class_counts.values, \n",
    "                   color=[colors[s] for s in class_counts.index], edgecolor='black')\n",
    "    axes[1, 1].set_xlabel('Substance')\n",
    "    axes[1, 1].set_ylabel('Sample Count')\n",
    "    axes[1, 1].set_title('Class Distribution', fontweight='bold', fontsize=14)\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add metrics text\n",
    "    metrics_text = f\"Silhouette: {separability_metrics['Silhouette_Score']:.3f}\\n\"\n",
    "    metrics_text += f\"Calinski-Harabasz: {separability_metrics['Calinski_Harabasz_Index']:.1f}\\n\"\n",
    "    metrics_text += f\"Davies-Bouldin: {separability_metrics['Davies_Bouldin_Index']:.3f}\"\n",
    "    axes[1, 1].text(0.02, 0.98, metrics_text, transform=axes[1, 1].transAxes, \n",
    "                    fontsize=10, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "    \n",
    "    plt.suptitle('Class Separability Analysis', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(data_path / '12_combined_separability.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"âœ“ Saved: 12_combined_separability.png\")\n",
    "    \n",
    "    # --- VERIFICATION ---\n",
    "    print(\"\\n>>> PRINT VERIFICATION:\")\n",
    "    print(f\"    âœ“ PCA analysis saved: 09_pca_analysis.png\")\n",
    "    print(f\"    âœ“ t-SNE analysis saved: 10_tsne_analysis.png\")\n",
    "    if UMAP_AVAILABLE:\n",
    "        print(f\"    âœ“ UMAP analysis saved: 11_umap_analysis.png\")\n",
    "    print(f\"    âœ“ Combined visualization saved: 12_combined_separability.png\")\n",
    "    print(f\"    âœ“ Silhouette Score: {separability_metrics['Silhouette_Score']:.4f}\")\n",
    "    \n",
    "\n",
    "    print(\"STEP 5: Feature Engineering\")\n",
    "    # Work with full dataset\n",
    "    df_eng = df.copy()\n",
    "    \n",
    "    # 5.1 Gas Resistance Features\n",
    "    print(\"\\n 5.1 Gas Resistance Features \")\n",
    "    \n",
    "    gas_res_cols = [c for c in numeric_cols if 'gas_res' in c]\n",
    "    if gas_res_cols:\n",
    "        # Mean, std, min, max across sensors\n",
    "        df_eng['gas_res_mean'] = df_eng[gas_res_cols].mean(axis=1)\n",
    "        df_eng['gas_res_std'] = df_eng[gas_res_cols].std(axis=1)\n",
    "        df_eng['gas_res_min'] = df_eng[gas_res_cols].min(axis=1)\n",
    "        df_eng['gas_res_max'] = df_eng[gas_res_cols].max(axis=1)\n",
    "        df_eng['gas_res_range'] = df_eng['gas_res_max'] - df_eng['gas_res_min']\n",
    "        df_eng['gas_res_cv'] = df_eng['gas_res_std'] / df_eng['gas_res_mean']  # Coefficient of variation\n",
    "        \n",
    "        # Ratios between sensors\n",
    "        if len(gas_res_cols) >= 2:\n",
    "            df_eng['gas_res_ratio_1_8'] = df_eng[gas_res_cols[0]] / df_eng[gas_res_cols[-1]].replace(0, np.nan)\n",
    "            df_eng['gas_res_ratio_max_min'] = df_eng['gas_res_max'] / df_eng['gas_res_min'].replace(0, np.nan)\n",
    "        \n",
    "        print(f\"  Created {8} gas resistance aggregate features\")\n",
    "    \n",
    "    # 5.2 Temperature & Humidity Features\n",
    "    print(\"\\n 5.2 Temperature & Humidity Features \")\n",
    "    \n",
    "    temp_cols = [c for c in numeric_cols if 'temp' in c]\n",
    "    hum_cols = [c for c in numeric_cols if 'hum' in c]\n",
    "    \n",
    "    if temp_cols:\n",
    "        df_eng['temp_mean'] = df_eng[temp_cols].mean(axis=1)\n",
    "        print(f\"  Created temp_mean feature\")\n",
    "    \n",
    "    if hum_cols:\n",
    "        df_eng['hum_mean'] = df_eng[hum_cols].mean(axis=1)\n",
    "        print(f\"  Created hum_mean feature\")\n",
    "    \n",
    "    # 5.3 Sensor Profile Features\n",
    "    print(\"\\n 5.3 Sensor Profile Features \")\n",
    "    \n",
    "    # Slope across gas resistance sensors (indicates sensor response pattern)\n",
    "    if len(gas_res_cols) >= 3:\n",
    "        # Calculate slope using linear regression across sensors\n",
    "        sensor_indices = np.arange(len(gas_res_cols))\n",
    "        \n",
    "        def calc_slope(row):\n",
    "            values = row[gas_res_cols].values.astype(float)\n",
    "            valid = ~pd.isna(values)\n",
    "            if valid.sum() >= 2:\n",
    "                try:\n",
    "                    return np.polyfit(sensor_indices[valid], values[valid], 1)[0]\n",
    "                except:\n",
    "                    return np.nan\n",
    "            return np.nan\n",
    "        \n",
    "        df_eng['gas_res_slope'] = df_eng.apply(calc_slope, axis=1)\n",
    "        print(f\"  Created gas_res_slope feature\")\n",
    "    \n",
    "    # 5.4 Save Engineered Features\n",
    "    engineered_cols = ['gas_res_mean', 'gas_res_std', 'gas_res_min', 'gas_res_max', \n",
    "                       'gas_res_range', 'gas_res_cv', 'gas_res_ratio_1_8', \n",
    "                       'gas_res_ratio_max_min', 'temp_mean', 'hum_mean', 'gas_res_slope']\n",
    "    engineered_cols = [c for c in engineered_cols if c in df_eng.columns]\n",
    "    \n",
    "    print(f\"\\n  Total engineered features: {len(engineered_cols)}\")\n",
    "\n",
    "    \n",
    "    print(\"\\n>>> PRINT VERIFICATION:\")\n",
    "    print(f\"    âœ“ Engineered features: {engineered_cols}\")\n",
    "    \n",
    "   \n",
    "\n",
    "    print(\"STEP 6: Prepare ML-Ready Data Structures\")\n",
    "    # Filter to substance measurements only\n",
    "    df_ml = df_eng[df_eng['measurement_type'] != 'air'].copy()\n",
    "    \n",
    "    # Define feature sets\n",
    "    all_features = numeric_cols + engineered_cols\n",
    "    all_features = [c for c in all_features if c in df_ml.columns]\n",
    "    \n",
    "    # Remove features with too many missing values\n",
    "    valid_features = []\n",
    "    for col in all_features:\n",
    "        missing_pct = df_ml[col].isnull().sum() / len(df_ml) * 100\n",
    "        if missing_pct < 30:\n",
    "            valid_features.append(col)\n",
    "    \n",
    "    print(f\"\\n  Total features: {len(all_features)}\")\n",
    "    print(f\"  Valid features (< 30% missing): {len(valid_features)}\")\n",
    "    \n",
    "    # Prepare X and y\n",
    "    X = df_ml[valid_features].copy()\n",
    "    y_substance = df_ml['substance'].copy()\n",
    "    \n",
    "    # Impute missing values\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=valid_features, index=X.index)\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y_substance)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X_imputed), columns=valid_features, index=X.index)\n",
    "    \n",
    "    # 6.1 Train/Validation/Test Split\n",
    "    print(\"\\n 6.1 Train/Validation/Test Split \")\n",
    "    \n",
    "    # First split: 80% train+val, 20% test\n",
    "    X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "        X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    # Second split: 75% train, 25% val (of the 80%)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_trainval, y_trainval, test_size=0.25, random_state=42, stratify=y_trainval\n",
    "    )\n",
    "    \n",
    "    print(f\"  Training set: {len(X_train)} samples ({len(X_train)/len(X_scaled)*100:.1f}%)\")\n",
    "    print(f\"  Validation set: {len(X_val)} samples ({len(X_val)/len(X_scaled)*100:.1f}%)\")\n",
    "    print(f\"  Test set: {len(X_test)} samples ({len(X_test)/len(X_scaled)*100:.1f}%)\")\n",
    "    \n",
    "    # 6.2 Save ML-Ready Datasets\n",
    "    print(\"\\n 6.2 Saving ML-Ready Datasets \")\n",
    "    \n",
    "    # Training set\n",
    "    train_df = X_train.copy()\n",
    "    train_df['target'] = y_train\n",
    "    train_df['substance'] = le.inverse_transform(y_train)\n",
    "    train_df.to_csv(data_path / 'ml_train_final.csv', index=False)\n",
    "    print(f\"  âœ“ Saved: ml_train_final.csv\")\n",
    "    \n",
    "    # Validation set\n",
    "    val_df = X_val.copy()\n",
    "    val_df['target'] = y_val\n",
    "    val_df['substance'] = le.inverse_transform(y_val)\n",
    "    val_df.to_csv(data_path / 'ml_val_final.csv', index=False)\n",
    "    print(f\"  âœ“ Saved: ml_val_final.csv\")\n",
    "    \n",
    "    # Test set\n",
    "    test_df = X_test.copy()\n",
    "    test_df['target'] = y_test\n",
    "    test_df['substance'] = le.inverse_transform(y_test)\n",
    "    test_df.to_csv(data_path / 'ml_test_final.csv', index=False)\n",
    "    print(f\"  âœ“ Saved: ml_test_final.csv\")\n",
    "    \n",
    "    # Full scaled dataset\n",
    "    full_df = X_scaled.copy()\n",
    "    full_df['target'] = y_encoded\n",
    "    full_df['substance'] = y_substance.values\n",
    "    full_df.to_csv(data_path / 'ml_full_scaled.csv', index=False)\n",
    "    print(f\"  âœ“ Saved: ml_full_scaled.csv\")\n",
    "    \n",
    "    # Unscaled (for tree-based models)\n",
    "    unscaled_df = X_imputed.copy()\n",
    "    unscaled_df['target'] = y_encoded\n",
    "    unscaled_df['substance'] = y_substance.values\n",
    "    unscaled_df.to_csv(data_path / 'ml_full_unscaled.csv', index=False)\n",
    "    print(f\"  âœ“ Saved: ml_full_unscaled.csv\")\n",
    "    \n",
    "    # 6.3 Save Metadata\n",
    "    print(\"\\n 6.3 Saving Metadata \")\n",
    "    \n",
    "    metadata = {\n",
    "        'feature_names': valid_features,\n",
    "        'n_features': len(valid_features),\n",
    "        'n_samples_total': len(X_scaled),\n",
    "        'n_train': len(X_train),\n",
    "        'n_val': len(X_val),\n",
    "        'n_test': len(X_test),\n",
    "        'class_labels': {name: int(idx) for idx, name in enumerate(le.classes_)},\n",
    "        'class_distribution': {\n",
    "            'train': {le.classes_[i]: int((y_train == i).sum()) for i in range(len(le.classes_))},\n",
    "            'val': {le.classes_[i]: int((y_val == i).sum()) for i in range(len(le.classes_))},\n",
    "            'test': {le.classes_[i]: int((y_test == i).sum()) for i in range(len(le.classes_))}\n",
    "        },\n",
    "        'engineered_features': engineered_cols,\n",
    "        'original_features': [c for c in valid_features if c not in engineered_cols],\n",
    "        'separability_metrics': {k: float(v) for k, v in separability_metrics.items()}\n",
    "    }\n",
    "    \n",
    "    with open(data_path / 'ml_metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"  âœ“ Saved: ml_metadata.json\")\n",
    "    \n",
    "    # Save scaler and encoder\n",
    "    import pickle\n",
    "    with open(data_path / 'scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    with open(data_path / 'label_encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(le, f)\n",
    "    with open(data_path / 'imputer.pkl', 'wb') as f:\n",
    "        pickle.dump(imputer, f)\n",
    "    print(f\"  âœ“ Saved: scaler.pkl, label_encoder.pkl, imputer.pkl\")\n",
    "    \n",
    "    # 6.4 Create K-Fold indices\n",
    "    print(\"\\n 6.4 Creating K-Fold Cross-Validation Indices \")\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold_indices = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_scaled, y_encoded)):\n",
    "        fold_indices.append({\n",
    "            'fold': fold + 1,\n",
    "            'train_indices': train_idx.tolist(),\n",
    "            'val_indices': val_idx.tolist()\n",
    "        })\n",
    "    \n",
    "    with open(data_path / 'kfold_indices.json', 'w') as f:\n",
    "        json.dump(fold_indices, f)\n",
    "    print(f\"  âœ“ Saved: kfold_indices.json (5-fold CV)\")\n",
    "    \n",
    "    # --- VERIFICATION ---\n",
    "    print(\"\\n>>> SECTION 6 VERIFICATION:\")\n",
    "    print(f\"    âœ“ Training samples: {len(X_train)}\")\n",
    "    print(f\"    âœ“ Validation samples: {len(X_val)}\")\n",
    "    print(f\"    âœ“ Test samples: {len(X_test)}\")\n",
    "    print(f\"    âœ“ Features: {len(valid_features)}\")\n",
    "    print(f\"    âœ“ Classes: {list(le.classes_)}\")\n",
    "    \n",
    "\n",
    "    print(\"SECTION 7: Data Quality Report\")\n",
    "    quality_report = {\n",
    "        'dataset_info': {\n",
    "            'total_samples': len(df),\n",
    "            'substance_samples': len(df_ml),\n",
    "            'air_samples': len(df) - len(df_ml),\n",
    "            'features_original': len(numeric_cols),\n",
    "            'features_engineered': len(engineered_cols),\n",
    "            'features_final': len(valid_features)\n",
    "        },\n",
    "        'missing_data': {\n",
    "            'total_missing_pct': (df[numeric_cols].isnull().sum().sum() / (len(df) * len(numeric_cols)) * 100),\n",
    "            'features_dropped': len(all_features) - len(valid_features)\n",
    "        },\n",
    "        'class_balance': {\n",
    "            'min_class_size': int(df_ml['substance'].value_counts().min()),\n",
    "            'max_class_size': int(df_ml['substance'].value_counts().max()),\n",
    "            'imbalance_ratio': df_ml['substance'].value_counts().max() / df_ml['substance'].value_counts().min()\n",
    "        },\n",
    "        'separability': separability_metrics,\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    with open(data_path / 'data_quality_report.json', 'w') as f:\n",
    "        json.dump(quality_report, f, indent=2)\n",
    "    \n",
    "    print(\"\\nData Quality Summary:\")\n",
    "    print(f\"  Total Samples: {quality_report['dataset_info']['total_samples']}\")\n",
    "    print(f\"  Final Features: {quality_report['dataset_info']['features_final']}\")\n",
    "    print(f\"  Missing Data: {quality_report['missing_data']['total_missing_pct']:.2f}%\")\n",
    "    print(f\"  Class Imbalance Ratio: {quality_report['class_balance']['imbalance_ratio']:.2f}\")\n",
    "    print(f\"  Silhouette Score: {separability_metrics['Silhouette_Score']:.4f}\")\n",
    "    \n",
    "    print(\"\\nRecommendations:\")\n",
    "    for rec in quality_report['recommendations']:\n",
    "        print(f\"  â€¢ {rec}\")\n",
    "    \n",
    "    # --- VERIFICATION ---\n",
    "    print(\"\\n>>> SECTION 7 VERIFICATION:\")\n",
    "    print(f\"    âœ“ Data quality report saved: data_quality_report.json\")\n",
    "    \n",
    "    # ==========================================================\n",
    "    # FINAL SUMMARY\n",
    "    # ==========================================================\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STEP-3 FINAL SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nğŸ“ All outputs saved to: {data_path.absolute()}\")\n",
    "    \n",
    "    print(\"\\nğŸ“Š STATISTICAL ANALYSIS:\")\n",
    "    print(\"   â€¢ detailed_substance_statistics.csv\")\n",
    "    print(\"   â€¢ detailed_feature_statistics.csv\")\n",
    "    print(\"   â€¢ anova_results.csv\")\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ FEATURE IMPORTANCE:\")\n",
    "    print(\"   â€¢ mutual_information_scores.csv\")\n",
    "    print(\"   â€¢ f_scores.csv\")\n",
    "    print(\"   â€¢ combined_feature_ranking.csv\")\n",
    "    \n",
    "    print(\"\\nğŸ–¼ï¸  VISUALIZATIONS:\")\n",
    "    print(\"   â€¢ 09_pca_analysis.png\")\n",
    "    print(\"   â€¢ 10_tsne_analysis.png\")\n",
    "    if UMAP_AVAILABLE:\n",
    "        print(\"   â€¢ 11_umap_analysis.png\")\n",
    "    print(\"   â€¢ 12_combined_separability.png\")\n",
    "    \n",
    "    print(\"\\nğŸ¤– ML-READY DATA:\")\n",
    "    print(\"   â€¢ ml_train_final.csv (60% of data)\")\n",
    "    print(\"   â€¢ ml_val_final.csv (20% of data)\")\n",
    "    print(\"   â€¢ ml_test_final.csv (20% of data)\")\n",
    "    print(\"   â€¢ ml_full_scaled.csv (standardized)\")\n",
    "    print(\"   â€¢ ml_full_unscaled.csv (for tree models)\")\n",
    "    print(\"   â€¢ ml_metadata.json\")\n",
    "    print(\"   â€¢ kfold_indices.json\")\n",
    "    print(\"   â€¢ scaler.pkl, label_encoder.pkl, imputer.pkl\")\n",
    "    \n",
    "    print(\"\\nğŸ“‹ QUALITY REPORT:\")\n",
    "    print(\"   â€¢ data_quality_report.json\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\">>> FINAL VERIFICATION: ALL SECTIONS PASSED âœ“\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"STEP-3 COMPLETE - Data is ready for ML model training! ğŸš€\")\n",
    "    \n",
    "    return df_ml\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f0a5b-42f1-46d1-8ea2-4daefad684ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57e8a91-e784-4b8d-8b26-5777a645837d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
